{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis project notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uncomment and run the cell below to automatically install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the search terms we need to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Search Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bts</td>\n",
       "      <td>16,723,304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>pewdiepie</td>\n",
       "      <td>16,495,659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>asmr</td>\n",
       "      <td>14,655,088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>billie eilish</td>\n",
       "      <td>13,801,247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>baby shark</td>\n",
       "      <td>12,110,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>old town road</td>\n",
       "      <td>10,456,524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>music</td>\n",
       "      <td>10,232,134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>badabun</td>\n",
       "      <td>10,188,997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>blackpink</td>\n",
       "      <td>9,580,131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>fortnite</td>\n",
       "      <td>9,117,342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #        Keyword Search Volume\n",
       "0   1            bts    16,723,304\n",
       "1   2      pewdiepie    16,495,659\n",
       "2   3           asmr    14,655,088\n",
       "3   4  billie eilish    13,801,247\n",
       "4   5     baby shark    12,110,100\n",
       "5   6  old town road    10,456,524\n",
       "6   7          music    10,232,134\n",
       "7   8        badabun    10,188,997\n",
       "8   9      blackpink     9,580,131\n",
       "9  10       fortnite     9,117,342"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = pd.read_csv(f'{os.getcwd()}\\\\Data\\\\search.tsv', sep='\\t') # obtained from https://ahrefs.com/blog/top-youtube-searches/\n",
    "terms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these search terms to get YouTube video links which we can then use to gather comments. To save some time (and some precious API quota) we'll use the top 50 searched terms\n",
    "\n",
    "This cell should not be run more than once a month. To make sure you never have to run it again, save the output to a file and load it for subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_video import get_videos\n",
    "# half = terms.head(50)\n",
    "# search_terms = half[\"Keyword\"]\n",
    "# urls = []\n",
    "\n",
    "# for term in search_terms:\n",
    "#     urls.append(get_videos(term))\n",
    "\n",
    "# links = list(set(item for sublist in urls for item in sublist))\n",
    "\n",
    "# with open(f\"{os.getcwd()}\\\\Data\\\\videos__.json\", \"w\") as f:\n",
    "#     json.dump(links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file contains <911> links\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{os.getcwd()}\\\\Data\\\\videos__.json\", \"r\") as f:\n",
    "    links = json.load(f)\n",
    "\n",
    "print(f\"This file contains <{len(links)}> links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot of links (and therefore a lot of data). However, taking into account the rate of analysis and the amount of comments per video, we realize that it's going to take a lot of time: many of these videos have around 20,000 comments minimum. Knowing that it takes this program around 30 seconds to analyze 100 comments, analyzing all comments would take days. \n",
    "\n",
    "Instead of waiting for 9 days, we can make a compromise: we can take a sample of 500 comments per video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our list of links, we can start collecting data. Let's do the fast part first: collecting the like/dislike ratio and saving it somewhere convenient for later. \n",
    "\n",
    "As before, this cell should ideally be run once only as it takes a bit of time - around 15 minutes. If the notebook needs to be restarted, you should load the csv file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_video import get_likes\n",
    "# start_time = time.time()\n",
    "\n",
    "# with open(\"{os.getcwd()}\\\\Data\\\\videos__.json\", \"r\") as f:\n",
    "#     links = json.load(f)\n",
    "# df_list = []\n",
    "\n",
    "# checkpoints = {int(len(links) * perc): perc * 100 for perc in [0.25, 0.5, 0.75, 0.9, 1]} # same as before\n",
    "\n",
    "# for i, link in enumerate(links):\n",
    "#     current_time = time.time()\n",
    "#     elapsed_time = current_time - start_time\n",
    "#     data = get_likes(link)\n",
    "#     df_list.append(data)\n",
    "#     time.sleep(1) # otherwise we will get an HTTP error\n",
    "#     if i + 1 in checkpoints:\n",
    "#             print(f\"{checkpoints[i+1] :.0f}% of the videos have been analyzed\\nTime elapsed: {elapsed_time :.2f} seconds\")  \n",
    "     \n",
    "# df = pd.DataFrame(df_list)\n",
    "# utils.save_analysis(df, \"top50_ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.youtube.com/watch?v=feLcMaiClOw</td>\n",
       "      <td>312761</td>\n",
       "      <td>19093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com/watch?v=tASwv-tkMlc</td>\n",
       "      <td>35026</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.youtube.com/watch?v=3U2dNKBM28o</td>\n",
       "      <td>126427</td>\n",
       "      <td>6010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.youtube.com/watch?v=1cPDfXU95Xw</td>\n",
       "      <td>23783</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.youtube.com/watch?v=pptIU_ZLlOo</td>\n",
       "      <td>4600</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.youtube.com/watch?v=ClQ-ymoXJZc</td>\n",
       "      <td>1079676</td>\n",
       "      <td>8874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.youtube.com/watch?v=8EJ3zbKTWQ8</td>\n",
       "      <td>11371069</td>\n",
       "      <td>1420624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.youtube.com/watch?v=t99KH0TR-J4</td>\n",
       "      <td>1435738</td>\n",
       "      <td>33092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.youtube.com/watch?v=DovdIspaqmw</td>\n",
       "      <td>146007</td>\n",
       "      <td>5324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.youtube.com/watch?v=DQQRjFzB8gY</td>\n",
       "      <td>870202</td>\n",
       "      <td>49922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           URL     likes  dislikes\n",
       "0  https://www.youtube.com/watch?v=feLcMaiClOw    312761     19093\n",
       "1  https://www.youtube.com/watch?v=tASwv-tkMlc     35026       165\n",
       "2  https://www.youtube.com/watch?v=3U2dNKBM28o    126427      6010\n",
       "3  https://www.youtube.com/watch?v=1cPDfXU95Xw     23783        18\n",
       "4  https://www.youtube.com/watch?v=pptIU_ZLlOo      4600         7\n",
       "5  https://www.youtube.com/watch?v=ClQ-ymoXJZc   1079676      8874\n",
       "6  https://www.youtube.com/watch?v=8EJ3zbKTWQ8  11371069   1420624\n",
       "7  https://www.youtube.com/watch?v=t99KH0TR-J4   1435738     33092\n",
       "8  https://www.youtube.com/watch?v=DovdIspaqmw    146007      5324\n",
       "9  https://www.youtube.com/watch?v=DQQRjFzB8gY    870202     49922"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes_df = pd.read_csv(f\"{os.getcwd()}\\\\Data\\\\analysis_top50_ratio.csv\", index_col = 0)\n",
    "likes_df = likes_df.reset_index()\n",
    "likes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good data, but it is not complete. We are still missing the ratio of likes to dislikes. Let's compute that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      16.381\n",
       "1     212.279\n",
       "2      21.036\n",
       "3    1321.278\n",
       "4     657.143\n",
       "5     121.667\n",
       "6       8.004\n",
       "7      43.386\n",
       "8      27.424\n",
       "9      17.431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes, dislikes = likes_df['likes'], likes_df['dislikes']\n",
    "\n",
    "ratios = []\n",
    "\n",
    "for like, dislike in zip(likes, dislikes):\n",
    "    if like == 0 or dislike == 0:\n",
    "        ratio = 0\n",
    "        ratios.append(ratio)\n",
    "    else:\n",
    "        ratio = round(like / dislike, 3)\n",
    "        ratios.append(ratio)\n",
    "\n",
    "ratios = pd.Series(ratios)\n",
    "ratios.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this list to our dataframe and compute the mean ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean like/dislike ratio = 123.55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.youtube.com/watch?v=feLcMaiClOw</td>\n",
       "      <td>312761</td>\n",
       "      <td>19093</td>\n",
       "      <td>16.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com/watch?v=tASwv-tkMlc</td>\n",
       "      <td>35026</td>\n",
       "      <td>165</td>\n",
       "      <td>212.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.youtube.com/watch?v=3U2dNKBM28o</td>\n",
       "      <td>126427</td>\n",
       "      <td>6010</td>\n",
       "      <td>21.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.youtube.com/watch?v=1cPDfXU95Xw</td>\n",
       "      <td>23783</td>\n",
       "      <td>18</td>\n",
       "      <td>1321.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.youtube.com/watch?v=pptIU_ZLlOo</td>\n",
       "      <td>4600</td>\n",
       "      <td>7</td>\n",
       "      <td>657.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.youtube.com/watch?v=ClQ-ymoXJZc</td>\n",
       "      <td>1079676</td>\n",
       "      <td>8874</td>\n",
       "      <td>121.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.youtube.com/watch?v=8EJ3zbKTWQ8</td>\n",
       "      <td>11371069</td>\n",
       "      <td>1420624</td>\n",
       "      <td>8.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.youtube.com/watch?v=t99KH0TR-J4</td>\n",
       "      <td>1435738</td>\n",
       "      <td>33092</td>\n",
       "      <td>43.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.youtube.com/watch?v=DovdIspaqmw</td>\n",
       "      <td>146007</td>\n",
       "      <td>5324</td>\n",
       "      <td>27.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.youtube.com/watch?v=DQQRjFzB8gY</td>\n",
       "      <td>870202</td>\n",
       "      <td>49922</td>\n",
       "      <td>17.431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           URL     likes  dislikes     ratio\n",
       "0  https://www.youtube.com/watch?v=feLcMaiClOw    312761     19093    16.381\n",
       "1  https://www.youtube.com/watch?v=tASwv-tkMlc     35026       165   212.279\n",
       "2  https://www.youtube.com/watch?v=3U2dNKBM28o    126427      6010    21.036\n",
       "3  https://www.youtube.com/watch?v=1cPDfXU95Xw     23783        18  1321.278\n",
       "4  https://www.youtube.com/watch?v=pptIU_ZLlOo      4600         7   657.143\n",
       "5  https://www.youtube.com/watch?v=ClQ-ymoXJZc   1079676      8874   121.667\n",
       "6  https://www.youtube.com/watch?v=8EJ3zbKTWQ8  11371069   1420624     8.004\n",
       "7  https://www.youtube.com/watch?v=t99KH0TR-J4   1435738     33092    43.386\n",
       "8  https://www.youtube.com/watch?v=DovdIspaqmw    146007      5324    27.424\n",
       "9  https://www.youtube.com/watch?v=DQQRjFzB8gY    870202     49922    17.431"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes_df['ratio'] = ratios.astype('float64') \n",
    "mean_ratio = ratios.mean()\n",
    "print(f\"Mean like/dislike ratio = {mean_ratio :.2f}\")\n",
    "likes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data in this dataframe as the baseline for the sentiment analysis.\n",
    "\n",
    "This process is going to take a long time to complete, even with multiprocessing. \n",
    "\n",
    "Let's start by collecting the comments.\n",
    "The results will be saved to a file, so there's no need to run this cell more than once if everything goes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_video import get_id, get_comments\n",
    "# import googleapiclient.discovery\n",
    "\n",
    "# urls = list(likes_df[\"URL\"])\n",
    "\n",
    "# target_dir = f\"{os.getcwd()}/Data\"\n",
    "# if not os.path.exists(target_dir):\n",
    "#     os.mkdir(target_dir)\n",
    "\n",
    "# for url in urls:\n",
    "#     video_id = get_id(url)\n",
    "#     try:\n",
    "#         comments = get_comments(url)\n",
    "#     except googleapiclient.errors.HttpError:\n",
    "#         continue\n",
    "#     utils.move_dir(f\"comments_{video_id}.json\", target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem: some of the files have very few comments, and we need at least 500. So, we will have to remove all files with less than 500 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dir = f\"{os.getcwd()}\\\\Data\"\n",
    "# trash = f\"{os.getcwd()}\\\\Trash\"\n",
    "\n",
    "# checkpoints = {int(len(links) * perc): perc * 100 for perc in [0.25, 0.5, 0.75, 0.9, 1]} # same as before\n",
    "\n",
    "# if not os.path.exists(trash):\n",
    "#     os.mkdir(trash)\n",
    "\n",
    "# data = os.listdir(target_dir)\n",
    "\n",
    "# start =  time.time()\n",
    "\n",
    "# for i, file in enumerate(data):\n",
    "#     current_time = time.time()\n",
    "#     elapsed_time = current_time - start\n",
    "#     filepath = os.path.join(target_dir, file)\n",
    "#     content = load_comments(filepath)\n",
    "#     if len(content) < 500:\n",
    "#         utils.move_dir(filename = file, destination = trash, source = target_dir)\n",
    "#     if i + 1 in checkpoints:\n",
    "#         print(f\"{checkpoints[i+1] :.0f}% of the videos have been analyzed\\nTime elapsed: {elapsed_time :.2f} seconds\") \n",
    "# trash_list = os.listdir(trash)\n",
    "\n",
    "# print(f\"<{len(trash_list)}> files moved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dir = f\"{os.getcwd()}\\\\Data\"\n",
    "\n",
    "# data = os.listdir(target_dir)\n",
    "# print(f\"This directory contains <{len(data)}> files\")\n",
    "# for file in data:\n",
    "#     filepath = os.path.join(target_dir, file)\n",
    "#     content = load_comments(filepath)\n",
    "#     print(len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have over 500 videos left, each with at least 500 comments. All that's left to do is run the sentiment analysis on each video. This is going to take a lot of time, which means we'll have to do it in batches. 100 videos per batch seems feasible, so let's do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run split_folder__.py \"F:\\Desktop\\Scuola\\Uni\\Y3\\S2\\Thesis\\Project\\Main project\\GetYoutubeComments\\Data\" \n",
    "# #Obtained from https://gist.github.com/zupo/5849843"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dir = f\"{os.getcwd()}\\\\Data\\\\Batch_1\"\n",
    "# data = os.listdir(target_dir)\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_video import load_comments\n",
    "\n",
    "# batch_6 = f\"{os.getcwd()}\\\\Data\\\\Batch_6\"\n",
    "# data_6 = os.listdir(batch_6)\n",
    "\n",
    "# os.chdir(batch_6)\n",
    "\n",
    "# for file in data_6:\n",
    "#     if file.startswith(\"analysis_\"): \n",
    "#         continue\n",
    "#     else:\n",
    "#         start = time.time()\n",
    "#         filename = f\"{batch_6}\\\\{file}\"\n",
    "#         content = load_comments(filename)\n",
    "\n",
    "#         with Pool() as p:\n",
    "#             tr_comment = p.map(utils.translate, content)\n",
    "\n",
    "#         bert_scores = utils.bert_classifier(content)\n",
    "\n",
    "#         df_list = []\n",
    "#         scores_list = []\n",
    "\n",
    "#         with Pool() as p:\n",
    "#             analysis1 = p.map(utils.vader_classifier, tr_comment)\n",
    "#             analysis2 = p.map(utils.textblob_classifier, tr_comment)\n",
    "#             scores_list.append(analysis1)\n",
    "#             scores_list.append(analysis2)\n",
    "\n",
    "#         for c, v_score, t_score in zip(content, scores_list[0], scores_list[1]):\n",
    "#             df_dict = {\n",
    "#                 \"Comment\": c,\n",
    "#                 \"Vader score\": v_score,\n",
    "#                 \"TextBlob score\": t_score,\n",
    "#             }\n",
    "#             df_list.append(df_dict)\n",
    "\n",
    "#         df = pd.DataFrame(df_list)\n",
    "\n",
    "#         series_bert = pd.Series(bert_scores)\n",
    "#         df[\"BERT score\"] = series_bert\n",
    "\n",
    "#         vader_labels = utils.generate_labels(df[\"Vader score\"], \"vader\")\n",
    "#         blob_labels = utils.generate_labels(df[\"TextBlob score\"], \"blob\")\n",
    "#         bert_labels = utils.generate_labels(df[\"BERT score\"], \"bert\")\n",
    "\n",
    "#         vader_labels_series = pd.Series(vader_labels)\n",
    "#         blob_labels_series = pd.Series(blob_labels)\n",
    "#         bert_labels_series = pd.Series(bert_labels)\n",
    "\n",
    "#         df[\"Vader label\"] = vader_labels_series\n",
    "#         df[\"TextBlob label\"] = blob_labels_series\n",
    "#         df[\"BERT label\"] = bert_labels_series\n",
    "\n",
    "#         df = df[\n",
    "#             [\n",
    "#                 \"Comment\",\n",
    "#                 \"Vader score\",\n",
    "#                 \"Vader label\",\n",
    "#                 \"TextBlob score\",\n",
    "#                 \"TextBlob label\",\n",
    "#                 \"BERT score\",\n",
    "#                 \"BERT label\",\n",
    "#             ]\n",
    "#         ]\n",
    "\n",
    "#         end = time.time()\n",
    "#         time_taken = end - start\n",
    "#         print(f\"Processed file <{file}>\\nTime taken: <{time_taken}> seconds\")\n",
    "#         utils.save_analysis(df, file)\n",
    "\n",
    "# os.chdir(\"F:\\\\Desktop\\\\Scuola\\\\Uni\\\\Y3\\\\S2\\\\Thesis\\\\Project\\\\Main project\\\\GetYoutubeComments\") # Reset cwd for next batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy and merge the CSV files for easier analysis of overall trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# folders = [\"Batch_1\", \"Batch_2\", \"Batch_3\", \"Batch_4\", \"Batch_5\", \"Batch_6\"]\n",
    "# data_dir = f\"{os.getcwd()}\\\\Data\"\n",
    "# target_dir = f\"{os.getcwd()}\\\\CSV\"\n",
    "\n",
    "# if not os.path.exists(target_dir):\n",
    "#     os.mkdir(target_dir)\n",
    "\n",
    "# for folder in folders:\n",
    "#     batch = f\"{data_dir}\\\\{folder}\"\n",
    "#     all_data = os.listdir(batch)\n",
    "#     for file in all_data:\n",
    "#         if file.endswith(\".csv\"):\n",
    "#             filename = os.path.join(batch, file)\n",
    "#             shutil.copy(filename, target_dir)            \n",
    "\n",
    "# files = os.listdir(target_dir)\n",
    "\n",
    "# combined_csv = pd.concat([pd.read_csv(f\"{target_dir}\\\\{f}\") for f in files])\n",
    "# combined_csv.to_csv(\"combined_csv.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# all_files = pd.read_csv(f\"{target_dir}\\\\combined_csv.csv\")\n",
    "# all_files.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dir = f\"{os.getcwd()}\\\\CSV\"\n",
    "# f = pd.read_csv(f\"{target_dir}\\\\combined_csv.csv\")\n",
    "\n",
    "# f = f.replace({\"Netural\": \"Neutral\"})\n",
    "\n",
    "# utils.save_analysis(f, filename = \"all_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a ratio of polarities for all videos for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratios: \n",
      "      Model  Positive   Neutral  Negative\n",
      "0     Vader  0.507953  0.357239  0.134808\n",
      "1  TextBlob  0.259344  0.646309  0.094347\n",
      "2      BERT  0.452432  0.244917  0.302651 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_dir = f\"{os.getcwd()}\\\\Data\\\\CSV\"\n",
    "all_files = pd.read_csv(f\"{target_dir}\\\\analysis_all_files.csv\")\n",
    "\n",
    "columns = [all_files[\"Vader label\"], all_files[\"TextBlob label\"], all_files[\"BERT label\"]]\n",
    "models = [\"Vader\", \"TextBlob\", \"BERT\"]\n",
    "df_list = []\n",
    "\n",
    "for column, model in zip(columns, models):\n",
    "    col = column.value_counts(normalize=True)\n",
    "    d = {\"Model\": model, \"Positive\": col['Positive'], \"Neutral\": col['Neutral'], \"Negative\": col['Negative']}\n",
    "    df_list.append(d)\n",
    "\n",
    "all_ratios = pd.DataFrame(df_list)\n",
    "print(\"Ratios: \")\n",
    "print(all_ratios, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader and BERT seem to be relatively close to each other in the ratio of their predictions, while TextBlob does not seem to follow the trend. \n",
    "Let's check if the predictions correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics: \n",
      "         Vader score  TextBlob score     BERT score\n",
      "count  339364.000000   339364.000000  339364.000000\n",
      "mean        0.255158        0.149271       0.204303\n",
      "std         0.448403        0.324981       0.459851\n",
      "min        -1.000000       -1.000000      -0.988600\n",
      "25%         0.000000        0.000000      -0.293853\n",
      "50%         0.090000        0.000000       0.308578\n",
      "75%         0.636900        0.350000       0.522687\n",
      "max         1.000000        1.000000       0.991498 \n",
      "\n",
      "Correlation matrix: \n",
      "                Vader score  TextBlob score  BERT score\n",
      "Vader score        1.000000        0.524019    0.447403\n",
      "TextBlob score     0.524019        1.000000    0.446724\n",
      "BERT score         0.447403        0.446724    1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Statistics: \")\n",
    "print(all_files.describe(), \"\\n\")\n",
    "\n",
    "print(\"Correlation matrix: \")\n",
    "print(all_files.corr(\"pearson\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to all have a slightly positive relationship, which means that they are correlated to each other (albeit not very much). With this in mind, let's start predicting the dislikes for each video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making a copy of the likes_df dataframe and adding the filename as a column of the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-6-AnhFG3cI', '-8VfKZCOo_I', '-BjZmE2gtdo', '-CmadmM5cOk', '-Hqp5Mwq3Zs', '-L1PrZf6OTU', '-eGEKFMHGr8', '-fWXLAkHmxA', '-s23GrMq-wA', '-tJYN-eG1zk', '0CWFO8cMxO4', '0HDdjwpPM3Y', '0zBSw5Rg41w', '1-xGerv5FOk', '1D5cxbDbKMc', '1GS7wxWPaxc', '1__CAdTJ5JU', '1_zgKRBrT0Y', '1ekZEVeXwek', '2S24-y0Ij3Y', '2Vv-BfVoq4g', '2Z0Put0teCM', '2cyzCReoNgU', '2fDVXFlHtdE', '2j9RcvNRywQ', '2sn45qRAEzE', '2zNB3CotyxM', '32si5cfrCNc', '34u9lJWBLMc', '3CxtK7-XtE0', '3R0AlwgcyDE', '3U2dNKBM28o', '3X9wEwulYhk', '3f2X_gIJ_u4', '3farSqm-AxM', '3pgWNs7lay0', '3qHhGmiAQoY', '3tmd-ClpJxA', '3vDuSr2VLmw', '4-TbQnONe_w', '4GuqB1BQVr4', '4Rgqc-9MPIY', '4UDaT5LVnmI', '4iQxG8ZjYO8', '4j725wScY0E', '58u-zkDLNPg', '5EQ8ctJmmUo', '5GJWxDKyk3A', '5TBW-uCkvtA', '5UMCrq-bBCg', '5X-Mrc2l1d0', '5eBBdBEbbXQ', '5kHq6hqgROc', '5ula1NjaHUA', '5unbAtbxvEM', '60ItHLz5WEA', '6DRkf3kZMXw', '6Dh-RL__uN4', '6G7GulQJUIU', '6Mgqbai3fKo', '6txSmc8napQ', '72UO0v5ESUo', '75PpxGzR7s0', '7Gs9zvu5FyE', '7JmprpRIsEY', '7MHlhHaLcTA', '7ZP_Fubqwcg', '7bXWtXxLzlk', '7wtfhZwyrcc', '7ysFgElQtjI', '86kKFYmAMxA', '87gWaABqGYs', '8CdcCD5V-d8', '8DytqFTwNSc', '8EFalFXALEA', '8EJ3zbKTWQ8', '8MEUWEvABTI', '8fqm3uht5xU', '8n-EoUv_qTc', '8qEf358A_EM', '8rODvcIcOF4', '8rZLnz-oo_c', '8w7chA9Wnuw', '8xg3vE8Ie_E', '8zdg-pDF10g', '91vECNhvmMY', '99bGsbcKQZ0', '9WbCfHutDSE', '9nClXHVdK4I', '9xG5aPvrS-k', '9ycKaF-64qg', 'A8KQhwmdZIw', 'Ac5tUDHlacU', 'Ah5MYGQBYRo', 'Amq-qlqbjYA', 'ApXoWvfEYVU', 'Az-mGR-CehY', 'B111sTzYFaQ', 'B6_iQvaIjXw', 'B96Jmhm2VEw', 'BBAyRBTfsOU', 'BFlG5sD540k', 'C3y6jGCXiUA', 'C6P6hrWyidA', 'CHVhwcOg6y8', 'CLrf5YCSsT8', 'CM4CkVFmTds', 'CPK_IdHe1Yg', 'CQAT5qdG8tI', 'CRYJshFLnAU', 'CVa3Hwh5IiI', 'CevxZvSJLk8', 'CjAHVcloVAo', 'Ck4xHocysLw', 'ClQ-ymoXJZc', 'CuklIb9d3fI', 'D9G1VOjN_84', 'DKYGY0HVoHE', 'DK_0jXPuIr0', 'DNB9IC0UXto', 'DQQRjFzB8gY', 'DiItGE3eAyQ', 'DjL9di-UIYc', 'Dkk9gvTmCXY', 'Dm9Zf1WYQ_A', 'DmKpePzn5-c', 'DovdIspaqmw', 'DpacO4GOIuQ', 'DyDfgMOUjCI', 'E10WufZ8NkY', 'E5MHEf5USUM', 'ED3xFj-jDI8', 'EIzcw5fVUtY', 'EXEqkVXFLhI', 'EbnH3VHzhu8', 'EgBJmlPo8Xw', 'EosMazKaPbU', 'FA2j9qlFyDU', 'FWI1BrV080g', 'Fdc446yiT-4', 'FjC5XHyK7hE', 'Fp8msa5uYsc', 'FttjE-tCZtI', 'FxQTY-W6GIo', 'G8APgeFfkAk', 'GU-vN1BHwhQ', 'G_BhUxx-cwk', 'GhOFMaqiAOE', 'Gl6ekgobG2k', 'GmHrjFIWl6U', 'Gsr_Jy56T4g', 'GtSRKwDCaZM', 'HDEC9cwdxGg', 'HDyXbkLgmUs', 'HUHC9tYz8ik', 'HXO0d0vOyFE', 'HZRMonTRP30', 'HgzGwKwLmgM', 'HhjHYkPQ8F0', 'Hr85jpejdLc', 'HzZ_urpj4As', 'IBaSizQyC5g', 'IC-CwFr29no', 'IHNzOHi8sJs', 'IJ-Bjm3I8B8', 'IdneKLhsWOQ', 'IhdsAPQ_jbs', 'IhugmviyP1g', 'Il0S8BoucSA', 'JGgnz0-w9TU', 'JGwWNGJdvx8', 'JIz0-6_mPIE', 'JSoGsO6n-qU', 'Jb2stN7kH28', 'JktwRXYTDYc', 'Jn1Uwsg3eRQ', 'JnJmBA8N-gY', 'Jqi4GBxmwgg', 'K0ibBPhiaG0', 'K7BIPr6MBLo', 'K7wmk7U4BhQ', 'KCwK2ECPDLA', 'KPYcso5TevE', 'KWGrPNqz4uc', 'KXw8CRapg7k', 'KYzMn1gjosY', 'KhXiCqeM7g0', 'KnyGqZgTU2A', 'Ks1Rgv1enjE', 'KtbVbCTzZd8', 'L40EFeYQmv8', 'L8eRzOYhLuw', 'LDnsJy2BasM', 'LEESPXcGbgQ', 'LH4Y1ZUUx2g', 'LTppaeecuIE', 'LeiFF0gvqcc', 'Lf2UNF4cYhg', 'LgesbsaGKlE', 'LwzN_v7GPMs', 'M-P4QBt-FWw', 'MBdVXkSdhwU', 'MPbUaIZAaeA', 'MbeNyTzksFQ', 'MrlNCKnRqjw', 'MuEhrNDetwk', 'MzzLsvB6oas', 'N0dbGGvsjf8', 'N97saaTR7JE', 'NIOIKdylRcM', 'NIRuH51h6m0', 'NLGdIWkhvAc', 'NPpELzyP4rw', 'NeRZ_tdD8KM', 'NgDb0fRCblo', 'NxilU56kPu0', 'NzkhFcZogc0', 'O3ElohqHEzQ', 'OBB-0OCCN9U', 'OD5sPf-Kgbc', 'OSUxrSe5GbI', 'OkmVcYtCqjw', 'OpD2c4Tu544', 'Ot3H4TvJlpo', 'P2KRKxAb2ek', 'PGJ43zaam_0', 'PHgc8Q6qTjc', 'PSO8vSizOYA', 'PV4NGwn_xdI', 'PivWY9wn5ps', 'Pkh8UtuejGw', 'PveR9ZdtLbY', 'Q2S7CDuBTOc', 'Q4a9uwV4nmo', 'Q527XDLEpfU', 'Q9Vjvz1NPN8', 'QD9HVyF3EEI', 'QMhVtPmPAW8', 'QNJL6nfu__Q', 'QO7e_RiMVQg', 'QVLvD5_6ld0', 'QYh6mYIJG2Y', 'QamGk6o9lZs', 'QcIy9NiNbmo', 'QnzbBzOGHPA', 'QsC1BinUPxo', 'RC37n6Fk3RE', 'RF78s4nphEY', 'RMc9qFrCqno', 'RNzQdcPWWX4', 'RUQl6YcMalg', 'RgzLnmTaCAU', 'RljdPIfnp0U', 'S2dRcipMCpw', 'S9bCLPwzSC0', 'SC4xMk98Pdc', 'SLsTskih7_I', 'SXiSVQZLje8', 'Sd4SJVsTulc', 'SntSCzTv_ko', 'SyMmSAAAXxg', 'TDVlDUAIz5k', 'TO-_3tck2tg', 'TOg39GWD5-E', 'TbuzMy8gHTE', 'Tc0tLGWIqxA', 'TkFHYODzRTs', 'TmKh7lAwnBI', 'TpHY1Oi068g', 'Tq60c_SaExc', 'TruV0K_lV4c', 'TxdwvrawHuo', 'U4_IiCusOOc', 'U4xPahDjY6k', 'UTHLKHL_whs', 'UXAdlaUhrlE', 'UYwF-jdcVjY', 'UceaB4D0jpo', 'Ud0O9bF_tCc', 'Udpjen4ON_8', 'UoweBBAiXUM', 'V-hwrOcuXOM', 'V1Pl8CzNzCw', 'V2hlQkVJZhE', 'V5M2WZiAy6k', 'VZOBjI-Vm8w', 'VqEbCxg2bNI', 'VronettmNRI', 'VuNIsY6JdUw', 'W0DM5lcj6mw', 'WA4iX5D9Z64', 'WAcnWtZjDWE', 'WEC4iwfN9zA', 'WG0WP6jgESs', 'WJN3AKboLuA', 'WMweEpGlu_U', 'WWCsGEarExg', 'WkgHkrM9fo0', 'X8sSXU-J8fI', 'XA2YEHn-A8Q', 'XAi3VTSdTxU', 'XBWyE1v8--I', 'XKDciUz-DpE', 'XbGs_qK2PQA', 'XcZX89Z0nVA', 'XekMkIJ843w', 'Xk0wdDTTPA0', 'XsX3ATc3FbA', 'Y2E71oe0aSM', 'Y2NkuFIlLEo', 'Y5WMMFQUP4I', 'YM3uJ6RtBZM', 'YVkUvmDQ3HY', 'YiTMyu-r_Ko', 'Z19DRZahG8c', 'Z3w5gVM_4y8', 'Z7l2Q25y0vU', 'Z9b0Hj-BfaM', 'ZCqG45PtfkE', 'ZJ4Qp1ietPc', 'ZSgVayPHuB8', 'ZXmjhRkPVFc', 'Z_MvkyuOJgk', 'ZcmoBlURJB4', 'Zi_XLOBDo_Y', 'ZnO8FHBdBC0', '_8cDGfrMh0o', '_EjtVzK5UFY', '_JOnM3jmY0o', '_Jtpf8N5IDE', '_KVZRCyEfnw', '_Yhyp-_hX2s', '_ae5Ap77b7k', '_ce7WX4l55A', '_pHTDbhutlY', '_wNsZEqpKUA', '_wtKe9Fq-eQ', 'a4-uIJJPrG4', 'aHnHwrJjR3U', 'aSjflT_J0Xo', 'acEOASYioGY', 'adsJy2abKs0', 'afUx6AIG3Tg', 'amhC8WYgNA4', 'anbrC6s0RD4', 'anfOF0KP4JE', 'au2n7VVGv_c', 'awkkyBH2zEo', 'ayd3yWr4tqU', 'bNTRIdraX8c', 'bPs0xFd4skY', 'bVmYuvJ_4v0', 'bX3S-_jUauc', 'ba7mB8oueCY', 'bibIWBTD_tE', 'binE0CCQgTo', 'bvWRMAU6V-c', 'bwmSjveL3Lc', 'bxnYFOixIoc', 'c4BLVznuWnU', 'c89M7PO0zY8', 'c8ztudg6N3k', 'cNhXH1etCwM', 'cW0jVkWAmOA', 'ca48oMV59LU', 'cqlRmt73PzE', 'crgFPl2eaPE', 'cwub1m1pg0c', 'cyW2ajAVyfA', 'd1oHZuF1k2I', 'dFp_b5DPIIo', 'dISNgvVpWlo', 'dNCWe_6HAM8', 'dTuSAkfuYLc', 'dZ0fwJojhrs', 'dmW68lzaaqs', 'doLMt10ytHY', 'dr_GAJZviR0', 'dsUXAEzaC3Q', 'dyRsYk0LyA8', 'e-ORhEE9VVg', 'e3C8Nh3DIwA', 'eJO5HU_7_1w', 'f5_wn8mexmM', 'f5omY8jVrSM', 'f7xYhYFi5aM', 'fJ9rUzIMcZQ', 'fKopy74weus', 'fUN2dGKxHFU', 'fUWn6BYrYK8', 'feLcMaiClOw', 'ffxKSjUwKdU', 'fpt5e0XHvwE', 'frNjBYk3SRw', 'gADgM89skZQ', 'gBRi6aZJGj4', 'gJhflBAwclA', 'gOMhN-hfMtY', 'gOsM-DYAEhY', 'gU2HqP4NxUs', 'gVbB7DCgfZs', 'gdZLi9oWNZg', 'gl1aHhXnN1k', 'gm3-m2CFVWM', 'gw1Es8tT5KI', 'gwMa6gpoE9I', 'gwkqL8PhCXA', 'gys9oDZj-MY', 'hD1YtmKXNb4', 'hHB1Ikzfpmc', 'h_D3VFfhvs4', 'heaYw-10T18', 'i0p1bmr0EmE', 'iMOkUyGnIIs', 'iVXVYsCSWd8', 'iXYyFqLi__8', 'ikebWCNAt0k', 'ioNng23DkIM', 'ixkoVwKQaJg', 'j440-D5JhjI', 'j5-yKhDd64s', 'j51hW59U1IM', 'j64oZLF443g', 'jR4vvMblYs0', 'jjW-0I4m2z0', 'jzYxbnHHhY4', 'k157GjJHvK4', 'k6ZoE4RrcDs', 'k8otG7pjZ50', 'kHLHSlExFis', 'kJQP7kiw5Fk', 'kOHB85vDuow', 'kPg3M4C9N9w', 'kPhpHvnnn0Q', 'kTJczUoc26U', 'kTlv5_Bs8aw', 'kWigaAi0kdE', 'kcYS_cUwr-8', 'kffacxfA7G4', 'kijpcUv-b8M', 'kt14c7sYTFA', 'ktvTqknDobU', 'l7ddWtP1DW8', 'lCiV4wACZ8w', 'lVZLHJIOVYY', 'liMkmUXdxCc', 'loSuMqwQA38', 'lp-EO5I60KA', 'ltZCnYMmVUs', 'luQ0JWcrsWg', 'm8E3Dh-gfhM', 'mIxlvVlOIS0', 'mLugzKRTf-0', 'mUlFZKFOFiM', 'mUngYa0rnig', 'mWRsgZuwf_8', 'mwHBdTqJd2g', 'mwWPnXhWKjA', 'mwatzFa-2I0', 'mwg299-9FMc', 'nDP9sOUI6ps', 'nJHYDkvRB2Y', 'nSDgHBxUbVQ', 'nfWlot6h_JM', 'nikm4zpKCPM', 'o-0ygW-B_gI', 'oKmJgeo7OLU', 'oPPO_wqlG9Q', 'oRdxUFDoQe0', 'omVvuL0lOes', 'orJSJGHjBLI', 'osTJc5e1Cyo', 'ow1QqW0jzTo', 'oyEuk8j8imI', 'p5rQDKoBd6U', 'pBOU8eKYnEM', 'pBuZEGYXA6E', 'pK060iUFWXg', 'pQCGBEdx8xQ', 'pTdjcLlElD4', 'pbMwTqkKSps', 'pekzpzNCNDQ', 'pqh4LfPeCYs', 'q0hyYWKXF0Q', 'qLBXcgdW7U8', 'qSPoWWOzIv8', 'qZRPXM3iF3g', 'qclWIFxq2F4', 'qdpXxGPqW-Y', 'qleJpzB4q7A', 'qov0ypksviU', 'qz2vodkoj7I', 'qzRIc14LC9U', 'r7qovpFAGrQ', 'rBNst8_bpUc', 'rSWHCZoX2mE', 'r_0JjYUe5jo', 'rnMIWj_t3Ow', 's4BAizuD7Tc', 'sJXZ9Dok7u8', 'sOnqjkJTMaA', 'sPopc47bshY', 's_eViPLUngU', 'sdAOoB5ML0Q', 'ss9ygQqqL2Q', 'ssq6X6alZ3w', 'ssvrReWSQpk', 'szCubxWdckw', 't2QL85K6eZU', 't99KH0TR-J4', 'tASwv-tkMlc', 'tCXGJQYZ9JA', 'tQ0yjYUFKAE', 'tQjsAJhsSw8', 'tQwVKr8rCYw', 't_jHrUE5IOk', 'tcYodQoapMg', 'tcrTQUVkUe0', 'tollGa3S0o8', 'ttWQK5VXskA', 'u7zcSHgKNLY', 'uDAjINEp8H8', 'uT3pPfdDrNs', 'ueaGjpweu38', 'ut4ZS7DE02s', 'uu3IBPlWq7I', 'v-sJ106h1dU', 'v3xwCkhmies', 'vMLk_T0PPbk', 'vPwaXytZcgI', 'vRXZj0DzXIA', 'vb8wloc4Xpw', 'viimfQi_pUw', 'vuDwCJN5UZU', 'vwnpFEh6DDI', 'w2Ov5jzm3j8', 'w4wdTdvM0dg', 'wI5zWSDiF3s', 'wJnBTPUQS5A', 'wMpqCRF7TKg', 'wX8W4x2j_0s', 'wXhTHyIgQ_U', 'wYZux3BMc5k', 'weeI1G46q0o', 'wfWkmURBNv8', 'whhXbtJgD_g', 'whwe0KD_rGw', 'wocKR3WDgAk', 'ws00k_lIQ9U', 'wwc3Wwj_6KI', 'x0N2kRv4Vfs', 'x34H5sk75XQ', 'xPQLcWV8lxU', 'xQOO2xGQ1Pc', 'xsZ9hpMgLvA', 'yDv0WSgXJVg', 'yUi_S6YWjZw', 'yduLp7emdX8', 'ynqibTQqlaY', 'ytQ5CYE1VZw', 'yybUX3f9J18', 'zAWsoFk2yVw', 'zLpVBsvcrAg', 'zOhDt1kin5o', 'zSq6Jk-sFFs', 'zea8Xc54tWc', 'zoYgCg6i9C8', 'zrJtFy51fRo', 'zrwTYozyzYA']\n"
     ]
    }
   ],
   "source": [
    "data_dir = f\"{os.getcwd()}\\\\Data\"\n",
    "csv_dir = f\"{data_dir}\\\\CSV\"\n",
    "\n",
    "csv_list = os.listdir(csv_dir)\n",
    "likes_df_short = likes_df.copy(deep = True)\n",
    "\n",
    "url_id = [url[-11:] for url in likes_df_short[\"URL\"]]\n",
    "file_id =[filename[18:-9] for filename in csv_list if filename.startswith(\"analysis_comments\")]\n",
    "\n",
    "test = sorted(list(set(url_id).intersection(file_id)))\n",
    "\n",
    "print(test)\n",
    "# likes_df_short[\"ID order\"] = pd.Series(url_id)\n",
    "# likes_df_short = likes_df_short.set_index(\"ID order\")\n",
    "# likes_df_short.loc[url_id]\n",
    "\n",
    "\n",
    "# likes_df_short.head(10)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_files = []\n",
    "id_lst = []\n",
    "for file in csv_list:\n",
    "    for url in urls:\n",
    "        url_id = url[-11:]\n",
    "        if file.startswith(\"analysis_comments\"):\n",
    "            id_ = file[18:-9]\n",
    "            if id_ == url_id:\n",
    "                target_files.append(file)\n",
    "                id_lst.append(url_id)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c3480dd823bfbce7c29a655c10633f6c16ea9c90c0e2bbbb7d47ca3445a085"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
